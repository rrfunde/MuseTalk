{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuseTalk on Google Colab\n",
    "\n",
    "This notebook sets up and runs MuseTalk on Google Colab with GPU acceleration.\n",
    "\n",
    "**Requirements:**\n",
    "- Google account\n",
    "- GPU runtime (free tier works)\n",
    "\n",
    "**Time to setup:** ~10-15 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Enable GPU Runtime\n",
    "\n",
    "**IMPORTANT:** Before running any cells:\n",
    "1. Click **Runtime** → **Change runtime type**\n",
    "2. Select **T4 GPU** (or any available GPU)\n",
    "3. Click **Save**\n",
    "\n",
    "Then verify GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone MuseTalk repository\n",
    "%cd /content\n",
    "!git clone https://github.com/TMElyralab/MuseTalk.git\n",
    "%cd MuseTalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (Colab usually has it, but ensure correct version)\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MMLab packages\n",
    "!pip install --no-cache-dir -U openmim\n",
    "!mim install mmengine\n",
    "!mim install \"mmcv==2.0.1\"\n",
    "!mim install \"mmdet==3.1.0\"\n",
    "!mim install \"mmpose==1.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Model Weights\n",
    "\n",
    "This will download ~8.5GB of model weights. Takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all model weights\n",
    "!bash download_weights.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all model files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'models/musetalk/musetalk.json',\n",
    "    'models/musetalk/pytorch_model.bin',\n",
    "    'models/musetalkV15/musetalk.json',\n",
    "    'models/musetalkV15/unet.pth',\n",
    "    'models/sd-vae/config.json',\n",
    "    'models/sd-vae/diffusion_pytorch_model.bin',\n",
    "    'models/whisper/config.json',\n",
    "    'models/whisper/pytorch_model.bin',\n",
    "    'models/dwpose/dw-ll_ucoco_384.pth',\n",
    "    'models/syncnet/latentsync_syncnet.pt',\n",
    "    'models/face-parse-bisent/79999_iter.pth',\n",
    "    'models/face-parse-bisent/resnet18-5c106cde.pth'\n",
    "]\n",
    "\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    if not exists:\n",
    "        all_present = False\n",
    "    print(f\"{'✅' if exists else '❌'} {f}\")\n",
    "\n",
    "print(f\"\\n{'✅ All models downloaded!' if all_present else '❌ Some models missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Launch Gradio Interface\n",
    "\n",
    "This creates a web interface you can use to upload videos and audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio app (will create a public URL)\n",
    "!python app.py --use_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Run Command-Line Inference\n",
    "\n",
    "### Upload Your Files First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your video and audio files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your video file:\")\n",
    "uploaded_video = files.upload()\n",
    "\n",
    "print(\"\\nUpload your audio file:\")\n",
    "uploaded_audio = files.upload()\n",
    "\n",
    "# Get filenames\n",
    "video_file = list(uploaded_video.keys())[0]\n",
    "audio_file = list(uploaded_audio.keys())[0]\n",
    "\n",
    "print(f\"\\nVideo: {video_file}\")\n",
    "print(f\"Audio: {audio_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with uploaded files\n",
    "!python -m scripts.inference \\\n",
    "  --video_path \"{video_file}\" \\\n",
    "  --audio_path \"{audio_file}\" \\\n",
    "  --result_dir results/custom \\\n",
    "  --unet_model_path models/musetalkV15/unet.pth \\\n",
    "  --unet_config models/musetalkV15/musetalk.json \\\n",
    "  --version v15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Test with Built-in Samples\n",
    "\n",
    "Use the included sample videos and audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with built-in test data\n",
    "!python -m scripts.inference \\\n",
    "  --inference_config configs/inference/test.yaml \\\n",
    "  --result_dir results/test \\\n",
    "  --unet_model_path models/musetalkV15/unet.pth \\\n",
    "  --unet_config models/musetalkV15/musetalk.json \\\n",
    "  --version v15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "!ls -lh results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download result video\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Find the output video (adjust path as needed)\n",
    "result_dir = 'results/test'  # or 'results/custom'\n",
    "for file in os.listdir(result_dir):\n",
    "    if file.endswith('.mp4'):\n",
    "        result_path = os.path.join(result_dir, file)\n",
    "        print(f\"Downloading: {result_path}\")\n",
    "        files.download(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Display Result in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result video in the notebook\n",
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "result_dir = 'results/test'  # or 'results/custom'\n",
    "for file in os.listdir(result_dir):\n",
    "    if file.endswith('.mp4'):\n",
    "        result_path = os.path.join(result_dir, file)\n",
    "        display(Video(result_path, width=640))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "```python\n",
    "# Use float16 precision\n",
    "!python app.py --use_float16\n",
    "# Or reduce batch size in configs\n",
    "```\n",
    "\n",
    "### GPU Not Available\n",
    "1. Go to **Runtime** → **Change runtime type**\n",
    "2. Select **GPU**\n",
    "3. Click **Save**\n",
    "4. Restart and run cells again\n",
    "\n",
    "### Models Not Downloading\n",
    "```python\n",
    "# Try manual download\n",
    "!wget https://huggingface.co/TMElyralab/MuseTalk/resolve/main/musetalkV15/unet.pth -P models/musetalkV15/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Tips\n",
    "\n",
    "1. **Use T4 GPU** (free tier) or upgrade to A100 for faster inference\n",
    "2. **Use float16** (`--use_float16`) to reduce memory usage\n",
    "3. **Keep videos short** (< 30 seconds) for faster processing\n",
    "4. **Use 25fps videos** for best results (matches training data)\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "- **T4 GPU (Free):** ~10-15fps\n",
    "- **V100 GPU:** ~20-30fps\n",
    "- **A100 GPU:** ~30fps+\n",
    "\n",
    "Processing a 10-second video typically takes 1-3 minutes on T4 GPU."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
